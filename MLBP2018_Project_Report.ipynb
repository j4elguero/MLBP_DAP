{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Basic Principles 2018 - Data Analysis Project Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *What is that tune? A classification method to identify song genres*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this report is to develop a method to identify the genres of a given list of songs. To develop this method, it is provided a dataset with different features of the songs, such as rhythm patterns, chroma, and values for the MFCCs. Consequently, it is followed by a workflow comprising three sections, preprocessing, modeling and results. These sections start by analyzing the data (Preprocessing), leaving it ready to apply the Machine Learning model (Modeling) and finally presenting the results obtained (Results). Through this process, different Machine Learning algorithms have been used to build the best solution. As a result, it is obtained an accuracy of 65% correct predictions in more than 6500 different songs with a log-loss of 0.17 by using Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation and discovery systems based on user preferences and activity have become increasingly popular across all different types of platforms, from retail (e.g. Amazon), to video (e.g. Netflix) and music (e.g. Spotify). These systems usually depend on a correct and precise categorization of products, which is not always something easy to achieve.\n",
    "\n",
    "The purpose of this project is to design and develop a machine learning solution to identify the music genre of a song given a set of attributes that describe its timbre, pitch and rhythm. The resulting model should be able to group music into categories that can later be used for recommendation or discovery.\n",
    "\n",
    "The problem of music genre classification is not an easy one because while the distinction between some genres is straightforward (e.g. heavy metal vs classical), others are harder to distinguish due to their similarity (e.g. rock vs blues).\n",
    "\n",
    "To address this problem, we will be dividing the workflow into three main sections:\n",
    "- **Preprocessing**: First, we will analyze the data provided and prepare it to be used by a machine learning algorithm.\n",
    "- **Modeling**: Once the data is ready, we will define different machine learning algorithms and fit them into the the training dataset to obtain a classification model, which we will use to predict the labels of the test dataset.\n",
    "- **Results**: Finally, we will upload the predicted labels of the test dataset to Kaggle, to have some feedback about the performance of our model. If the results are good, our job will be concluded. Otherwise, we will tune our model or select a new one and repeat the training process again.\n",
    "\n",
    "In the image below we can see the subtasks of each step while having an overall image of the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](img/diagram.svg)\n",
    "![Diagramxx](img/diagram_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part we will analyze the data and prepare it for the modeling phase. More specifically, we will look for data inconsistency, such as missing values (NA's), and highly correlated features, and we will standardize the data and reduce its dimensionality using Principal Component Analysis (PCA). After this whole process we will have a prepared dataset for the next phase (we will save this dataset in the **data_preprocessed** folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram_01](img/diagram_01.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and cleanup\n",
    "train_data = pd.read_csv(\"data/train_data.csv\", header=None)\n",
    "test_data = pd.read_csv(\"data/test_data.csv\", header=None)\n",
    "\n",
    "train_labels = pd.read_csv(\"data/train_labels.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA's in train_data dataset: 0\n",
      "Number of features: 264\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA's in correlation matrix: 2096\n",
      "The first 78 components explain 90.19 % of the variance\n"
     ]
    }
   ],
   "source": [
    "#Analysis of the input data\n",
    "\n",
    "# Are there any NA's?\n",
    "print(\"Number of NA's in train_data dataset:\" ,train_data.isna().sum().sum()) # 0 NA's\n",
    "\n",
    "# Number of features\n",
    "print(\"Number of features:\", train_data.shape[1])\n",
    "\n",
    "# Class distribution\n",
    "labels = train_labels.loc[:,0].values\n",
    "values = np.zeros(10)\n",
    "for i in np.arange(1,11):\n",
    "    count = 0\n",
    "    for j in range(labels.shape[0]):\n",
    "        if labels[j] == i :\n",
    "            count += 1\n",
    "    values[i-1] = count\n",
    "plt.bar(np.arange(1,11), values)\n",
    "plt.xticks(np.arange(1,11))\n",
    "plt.title(\"Class distribution\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.show()\n",
    "# As it can be seen on the graph, the majority of the samples belong to class 1\n",
    "# (Pop_Rock). This imbalance on the data will create a model that is really good\n",
    "# in indentifying samples that belong to this class.\n",
    "\n",
    "# Correlation matrix\n",
    "corr_mat = train_data.corr()\n",
    "\n",
    "# Are there any NA's in the correlation matrix?\n",
    "print(\"Number of NA's in correlation matrix:\", corr_mat.isna().sum().sum()) # 2,096 NA's\n",
    "\n",
    "# There are NA's in the correlation matrix which means there are features with\n",
    "# no deviation (std = 0).\n",
    "std = np.zeros(train_data.shape[1]) # Collect the std of each feature\n",
    "for j in range(train_data.shape[1]):\n",
    "    std[j] = np.std(train_data.loc[:,j].values)\n",
    "\n",
    "std_0 = np.where(std < 1e-3)[0] # Features 216, 217, 218, 219\n",
    "\n",
    "def remove_feat(df, feat):\n",
    "    # INPUT: Dataframe df, array of features indexes to remove.\n",
    "    # OUTPUT: Dataframe df without dropped features.\n",
    "    \n",
    "    num_remaining_feat = df.shape[1] - feat.shape[0]\n",
    "    df.drop(df.columns[feat], axis=1, inplace=True)\n",
    "    # Reasign index\n",
    "    df.columns = np.arange(0, num_remaining_feat)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Remove features with std = 0 since they have the same value for all the\n",
    "# samples and thus do not provide any differential information.\n",
    "train_data = remove_feat(train_data, std_0)\n",
    "test_data = remove_feat(test_data, std_0)\n",
    "\n",
    "# We recalculate the correlation matrix with the remaining features\n",
    "corr_mat = train_data.corr()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_feat = [] # Collect highly correlated features\n",
    "for j in range(1, corr_mat.shape[1]):\n",
    "    for i in range(0, j):\n",
    "        if np.abs(corr_mat.loc[i,j]) > 0.9:\n",
    "            high_corr_feat.append(j)\n",
    "            break\n",
    "high_corr_feat = np.array(high_corr_feat) # List to array\n",
    "\n",
    "# Remove high correlated features.\n",
    "train_data = remove_feat(train_data, high_corr_feat)\n",
    "test_data = remove_feat(test_data, high_corr_feat)\n",
    "\n",
    "# The number of features has been reduced down to\n",
    "num_feat = train_data.shape[1] # 193 features\n",
    "\n",
    "# Standardize data\n",
    "train_data_scaled = StandardScaler().fit_transform(train_data)\n",
    "test_data_scaled = StandardScaler().fit_transform(test_data)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(0.9)\n",
    "pca.fit(train_data_scaled)\n",
    "n_components = pca.n_components_\n",
    "explained_variance = np.round(np.sum(pca.explained_variance_ratio_) * 100, 2)\n",
    "print(\"The first\", n_components, \"components explain\", explained_variance, \"% of the variance\")\n",
    "\n",
    "train_data_pca = pca.transform(train_data_scaled)\n",
    "test_data_pca = pca.transform(test_data_scaled)\n",
    "\n",
    "# Convert transformed data to dataframe\n",
    "train_data_final = pd.DataFrame(data=train_data_pca)\n",
    "test_data_final = pd.DataFrame(data=test_data_pca)\n",
    "\n",
    "# Save the final datasets.\n",
    "train_data_final.to_csv(\"data_preprocessed/train_data.csv\", index=False)\n",
    "test_data_final.to_csv(\"data_preprocessed/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary:*\n",
    "\n",
    "- No NA's values were found in the dataset.\n",
    "- We removed 4 feautures that presented exactly the same value for all the samples and, therfore, did not provide any useful information to distinguish the genres.\n",
    "- We removed several features that were highly correlated, that is, that presented a correlation above 0.9 or below -0.9.\n",
    "- We analyzed the distribution of the labels and found that genre 1 (Pop_Rock) is the most dominant of the dataset. This creates an imbalance that will make the classification model be very good at identifying samples that belong to this class. (More comments about this imbalance in **Discussion/Conclusions** section).\n",
    "- We applied PCA and reduced the number of variables to 78 while still explaining 90.19% of the variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been preprocessed, we start applying different algorithmic models to it. We fit the model to the provided training set and we use it to predict the test data. Once we have the predictions, we export them to a *.csv* file inside the **output** folder and we upload them to Kaggle to test them.\n",
    "\n",
    "As this is a classification problem, we have decided to use three different machine learning algorithms:\n",
    "- Logistic regression\n",
    "- Random forest\n",
    "- Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram_02](img/diagram_02.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed data\n",
    "train_data = pd.read_csv(\"data_preprocessed/train_data.csv\")\n",
    "test_data = pd.read_csv(\"data_preprocessed/test_data.csv\")\n",
    "\n",
    "# Import train labels\n",
    "train_labels = pd.read_csv(\"data/train_labels.csv\", header=None)\n",
    "\n",
    "# Transform dataframes to arrays so that they can be fed to the models.\n",
    "X_train = train_data.values\n",
    "X_test = test_data.values\n",
    "y_train = train_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaime/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# MODEL-1: Logistic Regression\n",
    "\n",
    "# Import library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Configure and apply model\n",
    "model_1 = LogisticRegression(random_state=0, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "model_1.fit(X_train, y_train)\n",
    "y_test = model_1.predict(X_test)\n",
    "y_test_prob = model_1.predict_proba(X_test)\n",
    "\n",
    "# Output files\n",
    "\n",
    "n_samples = y_test.shape[0]\n",
    "\n",
    "# Accuracy\n",
    "out_array = np.zeros([n_samples, 2])\n",
    "out_df = pd.DataFrame(data=out_array, columns=[\"Sample_id\", \"Sample_label\"])\n",
    "out_df[\"Sample_id\"] = np.arange(1, n_samples + 1)\n",
    "out_df[\"Sample_label\"] = y_test\n",
    "out_df.to_csv(\"output/logistic_regression.csv\", index=False)\n",
    "\n",
    "# Probabilities\n",
    "out_array = np.zeros([n_samples, 11])\n",
    "columns = [\"Sample_id\"]\n",
    "for i in np.arange(1,11):\n",
    "    columns.append(\"Class_\" + str(i))\n",
    "out_df = pd.DataFrame(data=out_array, columns=columns)\n",
    "out_df[\"Sample_id\"] = np.arange(1, n_samples + 1)\n",
    "for j in np.arange(1,11):\n",
    "    out_df[columns[j]] = y_test_prob[:,j-1]\n",
    "out_df.to_csv(\"output/logistic_regression_prob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaime/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# MODEL-2: Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Configure and apply model\n",
    "model_2 = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 42)\n",
    "model_2.fit(X_train, y_train)\n",
    "y_test = model_2.predict(X_test)\n",
    "y_test_prob = model_2.predict_proba(X_test)\n",
    "\n",
    "# Output files\n",
    "\n",
    "n_samples = y_test.shape[0]\n",
    "\n",
    "# Accuracy\n",
    "out_array = np.zeros([n_samples, 2])\n",
    "out_df = pd.DataFrame(data=out_array, columns=[\"Sample_id\", \"Sample_label\"])\n",
    "out_df[\"Sample_id\"] = np.arange(1, n_samples + 1)\n",
    "out_df[\"Sample_label\"] = y_test\n",
    "out_df.to_csv(\"output/random_forest.csv\", index=False)\n",
    "\n",
    "# Probabilities\n",
    "out_array = np.zeros([n_samples, 11])\n",
    "columns = [\"Sample_id\"]\n",
    "for i in np.arange(1,11):\n",
    "    columns.append(\"Class_\" + str(i))\n",
    "out_df = pd.DataFrame(data=out_array, columns=columns)\n",
    "out_df[\"Sample_id\"] = np.arange(1, n_samples + 1)\n",
    "for j in np.arange(1,11):\n",
    "    out_df[columns[j]] = y_test_prob[:,j-1]\n",
    "out_df.to_csv(\"output/random_forest_prob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaime/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# MODEL-3: Support Vector Machine\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Configure and apply model\n",
    "model_3 = SVC(gamma=\"auto\", probability=True)\n",
    "model_3.fit(X_train, y_train)\n",
    "y_test = model_3.predict(X_test)\n",
    "y_test_prob = model_3.predict_proba(X_test)\n",
    "\n",
    "# Output files\n",
    "\n",
    "n_samples = y_test.shape[0]\n",
    "\n",
    "# Accuracy\n",
    "out_array = np.zeros([n_samples, 2])\n",
    "out_df = pd.DataFrame(data=out_array, columns=[\"Sample_id\", \"Sample_label\"])\n",
    "out_df[\"Sample_id\"] = np.arange(1, n_samples + 1)\n",
    "out_df[\"Sample_label\"] = y_test\n",
    "out_df.to_csv(\"output/svm.csv\", index=False)\n",
    "\n",
    "# Probabilities\n",
    "out_array = np.zeros([n_samples, 11])\n",
    "columns = [\"Sample_id\"]\n",
    "for i in np.arange(1,11):\n",
    "    columns.append(\"Class_\" + str(i))\n",
    "out_df = pd.DataFrame(data=out_array, columns=columns)\n",
    "out_df[\"Sample_id\"] = np.arange(1, n_samples + 1)\n",
    "for j in np.arange(1,11):\n",
    "    out_df[columns[j]] = y_test_prob[:,j-1]\n",
    "out_df.to_csv(\"output/svm_prob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we upload to Kaggle the results obtained after applying the different models to obtain the accuracy and the log-loss of the test dataset, and we compare these metrics with those of the train dataset. We compute also a confusion matrix of the train dataset for each of the models to analyze what wrong classifications were the most frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram_03](img/diagram_03.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "accuracy: 0.6903506761402705\n",
      "log-loss: 0.9501429470921838\n",
      "Random Forest:\n",
      "accuracy: 1.0\n",
      "log-loss: 0.27177420130372626\n",
      "SVM:\n",
      "accuracy: 0.856062342424937\n",
      "log-loss: 0.37591332054377385\n"
     ]
    }
   ],
   "source": [
    "# Performance on test data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def get_metrics(model):\n",
    "    # INPUT: trained model\n",
    "    # OUTPUT: printed accuracy and log-loss, return predicted labels for the training set\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred_proba = model.predict_proba(X_train)\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    loss = log_loss(y_train, y_pred_proba)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "    print(\"log-loss:\", loss)\n",
    "    return y_pred\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "y_pred_1 = get_metrics(model_1)\n",
    "print(\"Random Forest:\")\n",
    "y_pred_2 = get_metrics(model_2)\n",
    "print(\"SVM:\")\n",
    "y_pred_3 = get_metrics(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "[[1948  100   19   27   19   33    4   23    4    1]\n",
      " [ 169  393   20   16    4    8    1    2    4    1]\n",
      " [  42   18  240    3    3   12    1    0    7    0]\n",
      " [  80   14    5  136    4   11    1    0    0    2]\n",
      " [ 104   11    8   11   56    8    4    7    5    0]\n",
      " [  97   11   21   13    7  102    0    3    1    5]\n",
      " [  80   11    5    8    7    3   12    5    4    6]\n",
      " [ 121    1    1    2    3    3    2   62    0    0]\n",
      " [  10    5   15    1    3    7    0    0   51    0]\n",
      " [  45    1    2   12    4    2    1    7    0   12]]\n",
      "Random Forest:\n",
      "[[2178    0    0    0    0    0    0    0    0    0]\n",
      " [   0  618    0    0    0    0    0    0    0    0]\n",
      " [   0    0  326    0    0    0    0    0    0    0]\n",
      " [   0    0    0  253    0    0    0    0    0    0]\n",
      " [   0    0    0    0  214    0    0    0    0    0]\n",
      " [   0    0    0    0    0  260    0    0    0    0]\n",
      " [   0    0    0    0    0    0  141    0    0    0]\n",
      " [   0    0    0    0    0    0    0  195    0    0]\n",
      " [   0    0    0    0    0    0    0    0   92    0]\n",
      " [   0    0    0    0    0    0    0    0    0   86]]\n",
      "SVM:\n",
      "[[2153   16    6    1    0    2    0    0    0    0]\n",
      " [  64  543    7    2    0    2    0    0    0    0]\n",
      " [  19    6  300    0    1    0    0    0    0    0]\n",
      " [  38    6    1  203    0    5    0    0    0    0]\n",
      " [ 100    4    4    1  103    2    0    0    0    0]\n",
      " [  56    2    2    4    1  195    0    0    0    0]\n",
      " [  84   10    4    3    1    4   33    0    2    0]\n",
      " [  84    1    1    0    2    0    0  107    0    0]\n",
      " [   9    1   10    0    0    0    0    0   72    0]\n",
      " [  55    1    0    3    0    1    0    0    0   26]]\n"
     ]
    }
   ],
   "source": [
    "# Confussion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def get_confussion_matrix(y_pred):\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "print(\"Logistic Regression:\")\n",
    "get_confussion_matrix(y_pred_1)\n",
    "print(\"Random Forest:\")\n",
    "get_confussion_matrix(y_pred_2)\n",
    "print(\"SVM:\")\n",
    "get_confussion_matrix(y_pred_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results using the different models are:\n",
    "\n",
    "| Model | Test Acc. | Test Log-Loss | Train Acc. | Train Log-Loss |\n",
    "|-------|-----------|---------------|------------|----------------|\n",
    "| Logistic Regression | 0.65613 | 0.17826 | 0.69035 | 0.95014 |\n",
    "| Random Forest | 0.56597 | 0.21974 | 1 | 0.27177 |\n",
    "| Support Vector Machine | 0.62200 | 0.17637 | 0.85606 | 0.37716 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion/Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the performance of the three models on both the train and the test data, it is worth highlighting that:\n",
    "\n",
    "- The logistic regression model has the best overall performance, as it has the best test accuracy (0.65613) and a slightly superior log-loss (0.17826) than the SVM model (0.17637).\n",
    "- The random forest model is overfitted, as its accuracy on the training set is way higher (1) than the one obtained for the test set (0.56597).\n",
    "- The SVM model is also slightly overfitted, as its accuracy on the training set is higher (0.85606) than the one obtained for the test set (0.62200).\n",
    "\n",
    "As it was mentioned in the **Data analysis** section, the imbalance in the dataset caused by the great amount of samples that belong to class 1 (Pop_Rock) creates an algorithm that is biased in favor of class 1, that is, that will be more likely to classify samples as class 1. This can be easily seen by analyzing the confusion matrices - in all three cases, the values of the first column are greater than the values of the first row. To mitigate this effect, we decided to remove several samples from the training set that belonged to class 1. However, the performance on the test set dropped down when doing this. One possible explanation is that a big majority of songs belong to the Pop_Rock genre (class 1) and, therefore, a biased model towards this class will have better odds of correctly labeling samples.\n",
    "\n",
    "Even though the performance obtained by our best model is not outstanding, we are very close to Kaggle's best accuracy (0,01936 difference) and best log-loss (0,01163 difference).\n",
    "\n",
    "We believe that future improvements on the classification of music genre will come from increasing the number of samples and using complex neural networks, which continuously improve its performance as the number of samples increases.\n",
    "\n",
    "Finally, we can conclude that our project have managed to identify the music genre of the different samples, but the accuracy is far from good. Future improvements will be needed to create a system that is really realiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit Learn. Logistic Regression. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Scikit Learn. Random Forest. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- Scikit Learn. SVC. http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
